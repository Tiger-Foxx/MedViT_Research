{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209cb812",
   "metadata": {},
   "source": [
    "# üè• MedViT-CAMIL: Mode REAL (HyperKvasir Videos)\n",
    "\n",
    "**Context-Aware Multiple Instance Learning for Medical Video Analysis**\n",
    "\n",
    "Ce notebook ex√©cute le mode REAL sur Google Colab avec GPU.\n",
    "\n",
    "- **Dataset**: HyperKvasir Labeled Videos (~25 GB d'endoscopies)\n",
    "- **T√¢che**: Classification binaire (normal/pathologique)\n",
    "- **Comparaison**: Baseline (Average Pooling) vs MedViT-CAMIL (Gated Attention)\n",
    "\n",
    "---\n",
    "‚ö° **IMPORTANT**: \n",
    "1. Activez le GPU: `Runtime > Change runtime type > T4 GPU`\n",
    "2. Le dataset est volumineux (~25 GB), pr√©voir du temps de t√©l√©chargement\n",
    "\n",
    "üíæ **SAUVEGARDE**: Les r√©sultats sont automatiquement sauvegard√©s sur Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae6ebc",
   "metadata": {},
   "source": [
    "## 0Ô∏è‚É£ Montage Google Drive (PERSISTANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731a0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive pour sauvegarder les r√©sultats\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cr√©er le dossier de r√©sultats\n",
    "import os\n",
    "SAVE_DIR = '/content/drive/MyDrive/MedViT_Results/real'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"‚úÖ Dossier de sauvegarde: {SAVE_DIR}\")\n",
    "print(\"üìÅ Tous les fichiers seront automatiquement sauvegard√©s ici!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le GPU et l'espace disque\n",
    "!nvidia-smi\n",
    "!df -h /content\n",
    "\n",
    "import torch\n",
    "print(f\"\\n‚úÖ PyTorch {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PAS DE GPU! Activez: Runtime > Change runtime type > T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances\n",
    "!pip install -q timm tqdm matplotlib seaborn einops opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de09cf8e",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ T√©l√©chargement du Dataset HyperKvasir Videos\n",
    "\n",
    "**HyperKvasir** est un dataset d'endoscopies GI du Simula Research Lab.\n",
    "- 374 vid√©os labellis√©es de proc√©dures gastro-intestinales\n",
    "- Classes: findings normaux vs pathologiques (polypes, ulc√®res, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af9eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import shutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_DIR = '/content/data/hyperkvasir'\n",
    "ZIP_URL = 'https://datasets.simula.no/downloads/hyper-kvasir/hyper-kvasir-labeled-videos.zip'\n",
    "ZIP_PATH = '/content/hyper-kvasir-labeled-videos.zip'\n",
    "\n",
    "# Log de progression\n",
    "def log_progress(msg):\n",
    "    print(msg)\n",
    "    with open(f\"{SAVE_DIR}/download_log.txt\", 'a') as f:\n",
    "        f.write(f\"{datetime.now()}: {msg}\\n\")\n",
    "\n",
    "# T√©l√©charger si n√©cessaire\n",
    "if not os.path.exists(DATA_DIR) or len(os.listdir(DATA_DIR)) < 2:\n",
    "    log_progress(\"üì• T√©l√©chargement de HyperKvasir Videos (~25 GB)...\")\n",
    "    log_progress(\"‚è≥ Cela peut prendre 10-20 minutes selon la connexion...\")\n",
    "    !wget -q --show-progress -O $ZIP_PATH $ZIP_URL\n",
    "    \n",
    "    log_progress(\"üì¶ Extraction de l'archive...\")\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content/data/extracted')\n",
    "    \n",
    "    # Organiser en normal/abnormal\n",
    "    log_progress(\"üìÅ Organisation des donn√©es...\")\n",
    "    os.makedirs(f\"{DATA_DIR}/normal\", exist_ok=True)\n",
    "    os.makedirs(f\"{DATA_DIR}/abnormal\", exist_ok=True)\n",
    "    \n",
    "    NORMAL_KEYWORDS = ['normal', 'cecum', 'pylorus', 'z-line', 'retroflex']\n",
    "    ABNORMAL_KEYWORDS = ['polyp', 'ulcer', 'esophagitis', 'colitis', 'hemorrhoid', 'dyed']\n",
    "    \n",
    "    for video_path in glob.glob('/content/data/extracted/**/*.mp4', recursive=True):\n",
    "        folder_name = os.path.dirname(video_path).lower()\n",
    "        file_name = os.path.basename(video_path)\n",
    "        \n",
    "        is_normal = any(kw in folder_name for kw in NORMAL_KEYWORDS)\n",
    "        is_abnormal = any(kw in folder_name for kw in ABNORMAL_KEYWORDS)\n",
    "        \n",
    "        if is_abnormal:\n",
    "            dest = f\"{DATA_DIR}/abnormal/{file_name}\"\n",
    "        elif is_normal:\n",
    "            dest = f\"{DATA_DIR}/normal/{file_name}\"\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if not os.path.exists(dest):\n",
    "            shutil.copy2(video_path, dest)\n",
    "    \n",
    "    # Nettoyage\n",
    "    log_progress(\"üßπ Nettoyage...\")\n",
    "    !rm -rf /content/data/extracted\n",
    "    !rm -f $ZIP_PATH\n",
    "\n",
    "# Compter les fichiers\n",
    "normal_count = len(glob.glob(f\"{DATA_DIR}/normal/*.mp4\"))\n",
    "abnormal_count = len(glob.glob(f\"{DATA_DIR}/abnormal/*.mp4\"))\n",
    "\n",
    "dataset_info = {\n",
    "    'normal_videos': normal_count,\n",
    "    'abnormal_videos': abnormal_count,\n",
    "    'total': normal_count + abnormal_count\n",
    "}\n",
    "\n",
    "log_progress(f\"‚úÖ Dataset pr√™t: Normal={normal_count}, Abnormal={abnormal_count}\")\n",
    "\n",
    "# Sauvegarder info dataset\n",
    "with open(f\"{SAVE_DIR}/dataset_info.json\", 'w') as f:\n",
    "    json.dump(dataset_info, f, indent=2)\n",
    "print(f\"üíæ Info dataset sauvegard√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32656732",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ef6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration MODE REAL\n",
    "CONFIG = {\n",
    "    'MODE': 'real',\n",
    "    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'SEQ_LEN': 32,          # Frames extraites par vid√©o\n",
    "    'IMG_SIZE': 224,\n",
    "    'BATCH_SIZE': 8,        # R√©duit car vid√©os lourdes\n",
    "    'EPOCHS': 30,\n",
    "    'LR': 1e-4,\n",
    "    'WEIGHT_DECAY': 1e-5,\n",
    "    'NUM_CLASSES': 2,\n",
    "    'HIDDEN_DIM': 128,\n",
    "    'SEED': 42,\n",
    "    'DATA_DIR': DATA_DIR,\n",
    "    'SAVE_DIR': SAVE_DIR\n",
    "}\n",
    "\n",
    "# Reproductibilit√©\n",
    "torch.manual_seed(CONFIG['SEED'])\n",
    "np.random.seed(CONFIG['SEED'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CONFIG['SEED'])\n",
    "\n",
    "print(\"üìã CONFIGURATION REAL\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Sauvegarder la config imm√©diatement\n",
    "config_path = f\"{SAVE_DIR}/config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump({k: str(v) for k, v in CONFIG.items()}, f, indent=2)\n",
    "print(f\"\\nüíæ Config sauvegard√©e: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799557df",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Dataset: Real Video Loader (OpenCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9973f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "class RealVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset pour vraies vid√©os m√©dicales via OpenCV.\n",
    "    Extrait SEQ_LEN frames uniform√©ment r√©parties dans chaque vid√©o.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, split='train', seq_len=32, img_size=224):\n",
    "        self.seq_len = seq_len\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        ])\n",
    "        \n",
    "        # Scanner les fichiers\n",
    "        self.samples = []\n",
    "        for label, cls in enumerate(['normal', 'abnormal']):\n",
    "            cls_path = os.path.join(data_dir, cls)\n",
    "            if os.path.exists(cls_path):\n",
    "                for f in glob.glob(os.path.join(cls_path, '*.mp4')):\n",
    "                    self.samples.append((f, label))\n",
    "        \n",
    "        # Shuffle et split 80/20\n",
    "        np.random.shuffle(self.samples)\n",
    "        cut = int(0.8 * len(self.samples))\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.samples = self.samples[:cut]\n",
    "        else:\n",
    "            self.samples = self.samples[cut:]\n",
    "        \n",
    "        print(f\"[{split.upper()}] {len(self.samples)} vid√©os charg√©es\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def _extract_frames(self, video_path):\n",
    "        \"\"\"Extrait SEQ_LEN frames uniform√©ment de la vid√©o.\"\"\"\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total_frames == 0:\n",
    "            cap.release()\n",
    "            return None\n",
    "        \n",
    "        # Indices √† extraire (uniform√©ment r√©partis)\n",
    "        indices = np.linspace(0, total_frames - 1, self.seq_len, dtype=int)\n",
    "        \n",
    "        frame_idx = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            if frame_idx in indices:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(self.transform(frame))\n",
    "            \n",
    "            frame_idx += 1\n",
    "            if len(frames) >= self.seq_len:\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Padding si n√©cessaire\n",
    "        if len(frames) == 0:\n",
    "            return None\n",
    "        \n",
    "        while len(frames) < self.seq_len:\n",
    "            frames.append(frames[-1])  # R√©p√©ter la derni√®re frame\n",
    "        \n",
    "        return torch.stack(frames[:self.seq_len])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            video = self._extract_frames(path)\n",
    "            if video is None:\n",
    "                video = torch.zeros(self.seq_len, 3, self.img_size, self.img_size)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Erreur {path}: {e}\")\n",
    "            video = torch.zeros(self.seq_len, 3, self.img_size, self.img_size)\n",
    "        \n",
    "        return video, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Cr√©er les datasets\n",
    "train_dataset = RealVideoDataset(CONFIG['DATA_DIR'], 'train', CONFIG['SEQ_LEN'], CONFIG['IMG_SIZE'])\n",
    "val_dataset = RealVideoDataset(CONFIG['DATA_DIR'], 'val', CONFIG['SEQ_LEN'], CONFIG['IMG_SIZE'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "dataset_split = f\"Train: {len(train_dataset)} | Val: {len(val_dataset)}\"\n",
    "print(f\"\\nüìä {dataset_split}\")\n",
    "\n",
    "# Log\n",
    "with open(f\"{SAVE_DIR}/training_log.txt\", 'w') as f:\n",
    "    f.write(f\"MedViT-CAMIL REAL Training Log\\n\")\n",
    "    f.write(f\"Started: {datetime.now()}\\n\")\n",
    "    f.write(f\"Dataset: {dataset_split}\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ef1d9",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Mod√®les: Backbone MobileViT + Aggregateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604429f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class MobileViTBackbone(nn.Module):\n",
    "    \"\"\"Backbone MobileViT pr√©-entra√Æn√© (GEL√â).\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='mobilevit_s', pretrained=True):\n",
    "        super().__init__()\n",
    "        print(f\"[INFO] Chargement {model_name}...\")\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained, \n",
    "                                          num_classes=0, global_pool='avg')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            self.feature_dim = self.backbone(dummy).shape[-1]\n",
    "        \n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.backbone.eval()\n",
    "        print(f\"[INFO] Feature dim: {self.feature_dim}, Backbone GEL√â\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(x)\n",
    "        return features.view(B, T, -1)\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        self.backbone.eval()\n",
    "        return self\n",
    "\n",
    "\n",
    "class BaselineAvgPooling(nn.Module):\n",
    "    \"\"\"Baseline: Moyenne temporelle simple.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim, hidden_dim=128, num_classes=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        B, T, D = features.shape\n",
    "        projected = self.projection(features)\n",
    "        aggregated = projected.mean(dim=1)\n",
    "        logits = self.classifier(aggregated)\n",
    "        attention = torch.ones(B, T, device=features.device) / T\n",
    "        return logits, attention\n",
    "\n",
    "\n",
    "class ContextAwareGatedMIL(nn.Module):\n",
    "    \"\"\"CAMIL: Context-Aware Gated Attention MIL.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim, hidden_dim=128, num_classes=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.context_conv = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.attention_V = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.Tanh())\n",
    "        self.attention_U = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.Sigmoid())\n",
    "        self.attention_w = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        B, T, D = features.shape\n",
    "        \n",
    "        h = self.input_projection(features)\n",
    "        h_conv = self.context_conv(h.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        h = h + h_conv\n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        v = self.attention_V(h)\n",
    "        u = self.attention_U(h)\n",
    "        gated = v * u\n",
    "        \n",
    "        attention_scores = self.attention_w(gated).squeeze(-1)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        aggregated = torch.bmm(attention_weights.unsqueeze(1), h).squeeze(1)\n",
    "        logits = self.classifier(aggregated)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "\n",
    "class MedViTModel(nn.Module):\n",
    "    \"\"\"Mod√®le complet: Backbone + Aggregateur.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_camil=True, hidden_dim=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = MobileViTBackbone()\n",
    "        feature_dim = self.backbone.feature_dim\n",
    "        \n",
    "        if use_camil:\n",
    "            self.aggregator = ContextAwareGatedMIL(feature_dim, hidden_dim, num_classes)\n",
    "            self.name = \"MedViT-CAMIL\"\n",
    "        else:\n",
    "            self.aggregator = BaselineAvgPooling(feature_dim, hidden_dim, num_classes)\n",
    "            self.name = \"Baseline-AvgPool\"\n",
    "    \n",
    "    def forward(self, video):\n",
    "        features = self.backbone(video)\n",
    "        return self.aggregator(features)\n",
    "\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ae8fe",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Fonctions d'entra√Ænement (avec sauvegarde automatique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4123908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_message(msg, log_file=f\"{SAVE_DIR}/training_log.txt\"):\n",
    "    \"\"\"Affiche et sauvegarde un message.\"\"\"\n",
    "    print(msg)\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(msg + \"\\n\")\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for videos, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(videos)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, pred = logits.max(1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_attention = []\n",
    "    \n",
    "    for videos, labels in loader:\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "        logits, attention = model(videos)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, pred = logits.max(1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_attention.append(attention.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), correct / total, np.concatenate(all_attention)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config):\n",
    "    device = config['DEVICE']\n",
    "    save_dir = config['SAVE_DIR']\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total, trainable = count_params(model)\n",
    "    log_message(f\"\\n{'='*60}\")\n",
    "    log_message(f\"üîß {model.name}\")\n",
    "    log_message(f\"   Params: {total:,} total, {trainable:,} entra√Ænables\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=config['LR'], weight_decay=config['WEIGHT_DECAY']\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config['EPOCHS'])\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(config['EPOCHS']):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _ = evaluate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        msg = f\"Epoch {epoch+1}/{config['EPOCHS']} | Train: {train_loss:.4f} / {train_acc*100:.1f}% | Val: {val_loss:.4f} / {val_acc*100:.1f}%\"\n",
    "        log_message(msg)\n",
    "        \n",
    "        # Sauvegarder le meilleur mod√®le\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            model_path = f\"{save_dir}/{model.name}_best.pth\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            log_message(f\"   üíæ Nouveau meilleur mod√®le: {val_acc*100:.2f}%\")\n",
    "        \n",
    "        # Sauvegarder l'historique √† chaque epoch (protection d√©connexion)\n",
    "        history_path = f\"{save_dir}/{model.name}_history.json\"\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(history, f)\n",
    "    \n",
    "    log_message(f\"‚úÖ {model.name} Best Val Accuracy: {best_acc*100:.2f}%\")\n",
    "    return model, history, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a27eb8",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Entra√Ænement BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_message(f\"\\n{'='*60}\")\n",
    "log_message(\"üèÉ ENTRA√éNEMENT BASELINE (Average Pooling)\")\n",
    "log_message(f\"{'='*60}\")\n",
    "\n",
    "model_baseline = MedViTModel(use_camil=False, hidden_dim=CONFIG['HIDDEN_DIM'], num_classes=CONFIG['NUM_CLASSES'])\n",
    "model_baseline, history_baseline, best_baseline = train_model(model_baseline, train_loader, val_loader, CONFIG)\n",
    "\n",
    "print(f\"\\nüíæ Mod√®le baseline sauvegard√© dans {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b16fd5",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Entra√Ænement MedViT-CAMIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_message(f\"\\n{'='*60}\")\n",
    "log_message(\"üèÉ ENTRA√éNEMENT MedViT-CAMIL (Gated Attention)\")\n",
    "log_message(f\"{'='*60}\")\n",
    "\n",
    "model_camil = MedViTModel(use_camil=True, hidden_dim=CONFIG['HIDDEN_DIM'], num_classes=CONFIG['NUM_CLASSES'])\n",
    "model_camil, history_camil, best_camil = train_model(model_camil, train_loader, val_loader, CONFIG)\n",
    "\n",
    "print(f\"\\nüíæ Mod√®le CAMIL sauvegard√© dans {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf5b6f",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ R√©sultats Finaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5947c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = CONFIG['DEVICE']\n",
    "\n",
    "# Charger les meilleurs mod√®les\n",
    "model_baseline.load_state_dict(torch.load(f\"{SAVE_DIR}/Baseline-AvgPool_best.pth\"))\n",
    "model_camil.load_state_dict(torch.load(f\"{SAVE_DIR}/MedViT-CAMIL_best.pth\"))\n",
    "\n",
    "# √âvaluer\n",
    "_, val_acc_baseline, attention_baseline = evaluate(model_baseline, val_loader, criterion, device)\n",
    "_, val_acc_camil, attention_camil = evaluate(model_camil, val_loader, criterion, device)\n",
    "\n",
    "improvement = (val_acc_camil - val_acc_baseline) * 100\n",
    "\n",
    "log_message(f\"\\n{'='*60}\")\n",
    "log_message(\"üìä R√âSULTATS FINAUX (HyperKvasir Videos)\")\n",
    "log_message(f\"{'='*60}\")\n",
    "log_message(f\"Baseline (Avg Pool):  {val_acc_baseline*100:.2f}%\")\n",
    "log_message(f\"MedViT-CAMIL:         {val_acc_camil*100:.2f}%\")\n",
    "log_message(f\"Am√©lioration:         {improvement:+.2f}%\")\n",
    "log_message(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c131a",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Sauvegarde des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sultats complets\n",
    "results = {\n",
    "    'mode': 'real',\n",
    "    'dataset': 'HyperKvasir-Videos',\n",
    "    'timestamp': str(datetime.now()),\n",
    "    'config': {k: str(v) for k, v in CONFIG.items()},\n",
    "    'dataset_info': dataset_info,\n",
    "    'results': {\n",
    "        'baseline': {\n",
    "            'val_accuracy': val_acc_baseline,\n",
    "            'best_val_accuracy': best_baseline\n",
    "        },\n",
    "        'camil': {\n",
    "            'val_accuracy': val_acc_camil,\n",
    "            'best_val_accuracy': best_camil\n",
    "        },\n",
    "        'improvement': val_acc_camil - val_acc_baseline\n",
    "    },\n",
    "    'history': {\n",
    "        'baseline': history_baseline,\n",
    "        'camil': history_camil\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarder JSON\n",
    "results_path = f\"{SAVE_DIR}/results_real.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "log_message(f\"\\nüíæ R√©sultats sauvegard√©s: {results_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(json.dumps(results['results'], indent=2))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c1cd08",
   "metadata": {},
   "source": [
    "## üîü Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes d'entra√Ænement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(history_baseline['train_loss'], 'r-', label='Baseline Train', linewidth=2)\n",
    "ax.plot(history_baseline['val_loss'], 'r--', label='Baseline Val', linewidth=2)\n",
    "ax.plot(history_camil['train_loss'], 'g-', label='CAMIL Train', linewidth=2)\n",
    "ax.plot(history_camil['val_loss'], 'g--', label='CAMIL Val', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Training & Validation Loss', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot([a*100 for a in history_baseline['train_acc']], 'r-', label='Baseline Train', linewidth=2)\n",
    "ax.plot([a*100 for a in history_baseline['val_acc']], 'r--', label='Baseline Val', linewidth=2)\n",
    "ax.plot([a*100 for a in history_camil['train_acc']], 'g-', label='CAMIL Train', linewidth=2)\n",
    "ax.plot([a*100 for a in history_camil['val_acc']], 'g--', label='CAMIL Val', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Training & Validation Accuracy', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "curves_path = f\"{SAVE_DIR}/training_curves_real.png\"\n",
    "plt.savefig(curves_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"üíæ Sauvegard√©: {curves_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap d'attention\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "n_samples = min(15, len(attention_baseline))\n",
    "\n",
    "ax = axes[0]\n",
    "im = ax.imshow(attention_baseline[:n_samples], aspect='auto', cmap='Reds')\n",
    "ax.set_ylabel('Vid√©o', fontsize=12)\n",
    "ax.set_title('Baseline - Attention UNIFORME', fontsize=14)\n",
    "plt.colorbar(im, ax=ax, label='Poids')\n",
    "\n",
    "ax = axes[1]\n",
    "im = ax.imshow(attention_camil[:n_samples], aspect='auto', cmap='Greens')\n",
    "ax.set_xlabel('Frame (temps)', fontsize=12)\n",
    "ax.set_ylabel('Vid√©o', fontsize=12)\n",
    "ax.set_title('MedViT-CAMIL - Attention APPRISE (pics sur moments cl√©s)', fontsize=14)\n",
    "plt.colorbar(im, ax=ax, label='Poids')\n",
    "\n",
    "plt.tight_layout()\n",
    "heatmap_path = f\"{SAVE_DIR}/attention_heatmap_real.png\"\n",
    "plt.savefig(heatmap_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"üíæ Sauvegard√©: {heatmap_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f600c3",
   "metadata": {},
   "source": [
    "## üìã R√©sum√© Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_message(f\"\\n{'='*60}\")\n",
    "log_message(\"üìã R√âSUM√â FINAL\")\n",
    "log_message(f\"{'='*60}\")\n",
    "log_message(f\"Termin√©: {datetime.now()}\")\n",
    "log_message(f\"\")\n",
    "log_message(f\"M√âTRIQUES:\")\n",
    "log_message(f\"  Baseline Val Accuracy: {val_acc_baseline*100:.2f}%\")\n",
    "log_message(f\"  CAMIL Val Accuracy:    {val_acc_camil*100:.2f}%\")\n",
    "log_message(f\"  Am√©lioration:          {improvement:+.2f}%\")\n",
    "log_message(f\"\")\n",
    "log_message(f\"FICHIERS SAUVEGARD√âS dans {SAVE_DIR}:\")\n",
    "log_message(f\"  üìÑ config.json\")\n",
    "log_message(f\"  üìÑ results_real.json\")\n",
    "log_message(f\"  üìÑ training_log.txt\")\n",
    "log_message(f\"  üìÑ dataset_info.json\")\n",
    "log_message(f\"  üñºÔ∏è training_curves_real.png\")\n",
    "log_message(f\"  üñºÔ∏è attention_heatmap_real.png\")\n",
    "log_message(f\"  ü§ñ Baseline-AvgPool_best.pth\")\n",
    "log_message(f\"  ü§ñ MedViT-CAMIL_best.pth\")\n",
    "log_message(f\"{'='*60}\")\n",
    "\n",
    "# Lister les fichiers\n",
    "print(\"\\nüìÅ Fichiers dans Google Drive:\")\n",
    "!ls -la $SAVE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b4929",
   "metadata": {},
   "source": [
    "## üì• Instructions pour r√©cup√©rer les r√©sultats\n",
    "\n",
    "Les fichiers sont automatiquement sauvegard√©s dans **Google Drive** :\n",
    "`/MyDrive/MedViT_Results/real/`\n",
    "\n",
    "### Pour analyser les r√©sultats avec l'assistant:\n",
    "1. Va dans Google Drive ‚Üí MedViT_Results ‚Üí real\n",
    "2. T√©l√©charge les fichiers:\n",
    "   - `results_real.json` (m√©triques)\n",
    "   - `training_curves_real.png` (courbes)\n",
    "   - `attention_heatmap_real.png` (attention)\n",
    "3. D√©pose-les dans le dossier `results/` de ton projet local\n",
    "4. Demande √† l'assistant d'analyser les r√©sultats!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

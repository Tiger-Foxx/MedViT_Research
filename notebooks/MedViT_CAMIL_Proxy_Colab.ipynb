{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82316f76",
   "metadata": {},
   "source": [
    "# üè• MedViT-CAMIL: Mode PROXY (NoduleMNIST3D)\n",
    "\n",
    "**Context-Aware Multiple Instance Learning for Medical Video Analysis**\n",
    "\n",
    "Ce notebook ex√©cute le mode PROXY sur Google Colab avec GPU.\n",
    "\n",
    "- **Dataset**: NoduleMNIST3D (volumes CT 3D ‚Üí s√©quences 2D)\n",
    "- **T√¢che**: Classification binaire (nodule b√©nin/malin)\n",
    "- **Comparaison**: Baseline (Average Pooling) vs MedViT-CAMIL (Gated Attention)\n",
    "\n",
    "---\n",
    "‚ö° **IMPORTANT**: Activez le GPU avant d'ex√©cuter: `Runtime > Change runtime type > T4 GPU`\n",
    "\n",
    "üíæ **SAUVEGARDE**: Les r√©sultats sont automatiquement sauvegard√©s sur Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ab5e8",
   "metadata": {},
   "source": [
    "## 0Ô∏è‚É£ Montage Google Drive (PERSISTANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278fa376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive pour sauvegarder les r√©sultats\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cr√©er le dossier de r√©sultats\n",
    "import os\n",
    "SAVE_DIR = '/content/drive/MyDrive/MedViT_Results/proxy'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"‚úÖ Dossier de sauvegarde: {SAVE_DIR}\")\n",
    "print(\"üìÅ Tous les fichiers seront automatiquement sauvegard√©s ici!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06355776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\n‚úÖ PyTorch {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PAS DE GPU! Activez: Runtime > Change runtime type > T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances\n",
    "!pip install -q timm medmnist tqdm matplotlib seaborn einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0c1411",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import medmnist\n",
    "from medmnist import NoduleMNIST3D\n",
    "\n",
    "# Configuration MODE PROXY\n",
    "CONFIG = {\n",
    "    'MODE': 'proxy',\n",
    "    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'SEQ_LEN': 28,          # Slices du volume 3D\n",
    "    'IMG_SIZE': 224,\n",
    "    'BATCH_SIZE': 16,\n",
    "    'EPOCHS': 30,\n",
    "    'LR': 1e-4,\n",
    "    'WEIGHT_DECAY': 1e-5,\n",
    "    'NUM_CLASSES': 2,\n",
    "    'HIDDEN_DIM': 128,\n",
    "    'SEED': 42,\n",
    "    'SAVE_DIR': SAVE_DIR\n",
    "}\n",
    "\n",
    "# Reproductibilit√©\n",
    "torch.manual_seed(CONFIG['SEED'])\n",
    "np.random.seed(CONFIG['SEED'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CONFIG['SEED'])\n",
    "\n",
    "print(\"üìã CONFIGURATION PROXY\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Sauvegarder la config imm√©diatement\n",
    "config_path = f\"{SAVE_DIR}/config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump({k: str(v) for k, v in CONFIG.items()}, f, indent=2)\n",
    "print(f\"\\nüíæ Config sauvegard√©e: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886e797",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Dataset: NoduleMNIST3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1503ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "class ProxyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset proxy: NoduleMNIST3D transform√© en s√©quence 2D.\n",
    "    Volume 28x28x28 ‚Üí S√©quence de 28 images 224x224.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, split='train', img_size=224):\n",
    "        print(f\"[INFO] Chargement NoduleMNIST3D ({split})...\")\n",
    "        self.data = NoduleMNIST3D(split=split, download=True, as_rgb=False)\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Gray‚ÜíRGB\n",
    "            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        ])\n",
    "        print(f\"[INFO] {len(self.data)} volumes charg√©s\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        volume, label = self.data[idx]\n",
    "        volume = np.array(volume)\n",
    "        \n",
    "        if volume.ndim == 4:\n",
    "            volume = volume[0]\n",
    "        \n",
    "        frames = []\n",
    "        for z in range(volume.shape[0]):\n",
    "            slice_2d = volume[z]\n",
    "            slice_2d = ((slice_2d - slice_2d.min()) / (slice_2d.max() - slice_2d.min() + 1e-8) * 255).astype(np.uint8)\n",
    "            frames.append(self.transform(slice_2d))\n",
    "        \n",
    "        video = torch.stack(frames)\n",
    "        label = torch.tensor(int(label.item() > 0), dtype=torch.long)\n",
    "        return video, label\n",
    "\n",
    "# Cr√©er les datasets\n",
    "train_dataset = ProxyDataset('train', CONFIG['IMG_SIZE'])\n",
    "val_dataset = ProxyDataset('val', CONFIG['IMG_SIZE'])\n",
    "test_dataset = ProxyDataset('test', CONFIG['IMG_SIZE'])\n",
    "\n",
    "# num_workers=0 pour √©viter les erreurs multiprocessing sur Colab\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "dataset_info = f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\"\n",
    "print(f\"\\nüìä {dataset_info}\")\n",
    "\n",
    "# Log\n",
    "with open(f\"{SAVE_DIR}/training_log.txt\", 'w') as f:\n",
    "    f.write(f\"MedViT-CAMIL PROXY Training Log\\n\")\n",
    "    f.write(f\"Started: {datetime.now()}\\n\")\n",
    "    f.write(f\"Dataset: {dataset_info}\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ae90f",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Mod√®les: Backbone MobileViT + Aggregateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2cda7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class MobileViTBackbone(nn.Module):\n",
    "    \"\"\"Backbone MobileViT pr√©-entra√Æn√© (GEL√â).\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='mobilevit_s', pretrained=True):\n",
    "        super().__init__()\n",
    "        print(f\"[INFO] Chargement {model_name}...\")\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained, \n",
    "                                          num_classes=0, global_pool='avg')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            self.feature_dim = self.backbone(dummy).shape[-1]\n",
    "        \n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.backbone.eval()\n",
    "        print(f\"[INFO] Feature dim: {self.feature_dim}, Backbone GEL√â\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(x)\n",
    "        return features.view(B, T, -1)\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        self.backbone.eval()\n",
    "        return self\n",
    "\n",
    "\n",
    "class BaselineAvgPooling(nn.Module):\n",
    "    \"\"\"Baseline: Moyenne temporelle simple.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim, hidden_dim=128, num_classes=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        B, T, D = features.shape\n",
    "        projected = self.projection(features)\n",
    "        aggregated = projected.mean(dim=1)  # Simple average\n",
    "        logits = self.classifier(aggregated)\n",
    "        attention = torch.ones(B, T, device=features.device) / T  # Uniform\n",
    "        return logits, attention\n",
    "\n",
    "\n",
    "class ContextAwareGatedMIL(nn.Module):\n",
    "    \"\"\"CAMIL: Context-Aware Gated Attention MIL.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim, hidden_dim=128, num_classes=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Conv1D pour contexte temporel (KERNEL=3)\n",
    "        self.context_conv = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Gated Attention branches\n",
    "        self.attention_V = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.Tanh())\n",
    "        self.attention_U = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.Sigmoid())\n",
    "        self.attention_w = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        B, T, D = features.shape\n",
    "        \n",
    "        # Projection\n",
    "        h = self.input_projection(features)\n",
    "        \n",
    "        # Contexte temporel (Conv1D)\n",
    "        h_conv = self.context_conv(h.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        h = h + h_conv  # Residual\n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        # Gated Attention\n",
    "        v = self.attention_V(h)  # Tanh branch\n",
    "        u = self.attention_U(h)  # Sigmoid branch\n",
    "        gated = v * u            # Element-wise gating\n",
    "        \n",
    "        attention_scores = self.attention_w(gated).squeeze(-1)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # Weighted aggregation\n",
    "        aggregated = torch.bmm(attention_weights.unsqueeze(1), h).squeeze(1)\n",
    "        logits = self.classifier(aggregated)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "\n",
    "class MedViTModel(nn.Module):\n",
    "    \"\"\"Mod√®le complet: Backbone + Aggregateur.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_camil=True, hidden_dim=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = MobileViTBackbone()\n",
    "        feature_dim = self.backbone.feature_dim\n",
    "        \n",
    "        if use_camil:\n",
    "            self.aggregator = ContextAwareGatedMIL(feature_dim, hidden_dim, num_classes)\n",
    "            self.name = \"MedViT-CAMIL\"\n",
    "        else:\n",
    "            self.aggregator = BaselineAvgPooling(feature_dim, hidden_dim, num_classes)\n",
    "            self.name = \"Baseline-AvgPool\"\n",
    "    \n",
    "    def forward(self, video):\n",
    "        features = self.backbone(video)\n",
    "        return self.aggregator(features)\n",
    "\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c801b025",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Fonctions d'entra√Ænement (avec sauvegarde automatique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f233a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_message(msg, log_file=f\"{SAVE_DIR}/training_log.txt\"):\n",
    "    \"\"\"Affiche et sauvegarde un message.\"\"\"\n",
    "    print(msg)\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(msg + \"\\n\")\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for videos, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(videos)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, pred = logits.max(1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_attention = []\n",
    "    \n",
    "    for videos, labels in loader:\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "        logits, attention = model(videos)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, pred = logits.max(1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_attention.append(attention.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), correct / total, np.concatenate(all_attention)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config):\n",
    "    device = config['DEVICE']\n",
    "    save_dir = config['SAVE_DIR']\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total, trainable = count_params(model)\n",
    "    log_message(f\"\\n{'='*60}\")\n",
    "    log_message(f\"üîß {model.name}\")\n",
    "    log_message(f\"   Params: {total:,} total, {trainable:,} entra√Ænables\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=config['LR'], weight_decay=config['WEIGHT_DECAY']\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config['EPOCHS'])\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(config['EPOCHS']):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _ = evaluate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        msg = f\"Epoch {epoch+1}/{config['EPOCHS']} | Train: {train_loss:.4f} / {train_acc*100:.1f}% | Val: {val_loss:.4f} / {val_acc*100:.1f}%\"\n",
    "        log_message(msg)\n",
    "        \n",
    "        # Sauvegarder le meilleur mod√®le\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            model_path = f\"{save_dir}/{model.name}_best.pth\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            log_message(f\"   üíæ Nouveau meilleur mod√®le: {val_acc*100:.2f}%\")\n",
    "        \n",
    "        # Sauvegarder l'historique √† chaque epoch (protection d√©connexion)\n",
    "        history_path = f\"{save_dir}/{model.name}_history.json\"\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(history, f)\n",
    "    \n",
    "    log_message(f\"‚úÖ {model.name} Best Val Accuracy: {best_acc*100:.2f}%\")\n",
    "    return model, history, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a39b6",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Entra√Ænement BASELINE (Average Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc434aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_message(f\"\\n{'='*60}\")\n",
    "log_message(\"üèÉ ENTRA√éNEMENT BASELINE (Average Pooling)\")\n",
    "log_message(f\"{'='*60}\")\n",
    "\n",
    "model_baseline = MedViTModel(use_camil=False, hidden_dim=CONFIG['HIDDEN_DIM'], num_classes=CONFIG['NUM_CLASSES'])\n",
    "model_baseline, history_baseline, best_baseline = train_model(model_baseline, train_loader, val_loader, CONFIG)\n",
    "\n",
    "print(f\"\\nüíæ Mod√®le baseline sauvegard√© dans {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e2979",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Entra√Ænement MedViT-CAMIL (Gated Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df698ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_message(f\"\\n{'='*60}\")\n",
    "log_message(\"üèÉ ENTRA√éNEMENT MedViT-CAMIL (Gated Attention)\")\n",
    "log_message(f\"{'='*60}\")\n",
    "\n",
    "model_camil = MedViTModel(use_camil=True, hidden_dim=CONFIG['HIDDEN_DIM'], num_classes=CONFIG['NUM_CLASSES'])\n",
    "model_camil, history_camil, best_camil = train_model(model_camil, train_loader, val_loader, CONFIG)\n",
    "\n",
    "print(f\"\\nüíæ Mod√®le CAMIL sauvegard√© dans {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94106765",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ √âvaluation Finale sur Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd3d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = CONFIG['DEVICE']\n",
    "\n",
    "# Charger les meilleurs mod√®les\n",
    "model_baseline.load_state_dict(torch.load(f\"{SAVE_DIR}/Baseline-AvgPool_best.pth\"))\n",
    "model_camil.load_state_dict(torch.load(f\"{SAVE_DIR}/MedViT-CAMIL_best.pth\"))\n",
    "\n",
    "# √âvaluer sur TEST set\n",
    "log_message(f\"\\n{'='*60}\")\n",
    "log_message(\"üìä √âVALUATION FINALE SUR TEST SET\")\n",
    "log_message(f\"{'='*60}\")\n",
    "\n",
    "_, test_acc_baseline, attention_baseline = evaluate(model_baseline, test_loader, criterion, device)\n",
    "_, test_acc_camil, attention_camil = evaluate(model_camil, test_loader, criterion, device)\n",
    "\n",
    "# Calculer aussi les accuracies de validation finales\n",
    "_, val_acc_baseline, _ = evaluate(model_baseline, val_loader, criterion, device)\n",
    "_, val_acc_camil, _ = evaluate(model_camil, val_loader, criterion, device)\n",
    "\n",
    "improvement = (test_acc_camil - test_acc_baseline) * 100\n",
    "\n",
    "log_message(f\"\\nR√âSULTATS (NoduleMNIST3D):\")\n",
    "log_message(f\"  Baseline (Avg Pool) - Val: {val_acc_baseline*100:.2f}% | Test: {test_acc_baseline*100:.2f}%\")\n",
    "log_message(f\"  MedViT-CAMIL        - Val: {val_acc_camil*100:.2f}% | Test: {test_acc_camil*100:.2f}%\")\n",
    "log_message(f\"  Am√©lioration Test: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe180c",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Sauvegarde des R√©sultats Finaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sultats complets\n",
    "results = {\n",
    "    'mode': 'proxy',\n",
    "    'dataset': 'NoduleMNIST3D',\n",
    "    'timestamp': str(datetime.now()),\n",
    "    'config': {k: str(v) for k, v in CONFIG.items()},\n",
    "    'results': {\n",
    "        'baseline': {\n",
    "            'val_accuracy': val_acc_baseline,\n",
    "            'test_accuracy': test_acc_baseline,\n",
    "            'best_val_accuracy': best_baseline\n",
    "        },\n",
    "        'camil': {\n",
    "            'val_accuracy': val_acc_camil,\n",
    "            'test_accuracy': test_acc_camil,\n",
    "            'best_val_accuracy': best_camil\n",
    "        },\n",
    "        'improvement_test': test_acc_camil - test_acc_baseline,\n",
    "        'improvement_val': val_acc_camil - val_acc_baseline\n",
    "    },\n",
    "    'history': {\n",
    "        'baseline': history_baseline,\n",
    "        'camil': history_camil\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarder JSON\n",
    "results_path = f\"{SAVE_DIR}/results_proxy.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "log_message(f\"\\nüíæ R√©sultats sauvegard√©s: {results_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(json.dumps(results['results'], indent=2))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba023a",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Visualisations (sauvegard√©es automatiquement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942718ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes d'entra√Ænement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax = axes[0]\n",
    "ax.plot(history_baseline['train_loss'], 'r-', label='Baseline Train', linewidth=2)\n",
    "ax.plot(history_baseline['val_loss'], 'r--', label='Baseline Val', linewidth=2)\n",
    "ax.plot(history_camil['train_loss'], 'g-', label='CAMIL Train', linewidth=2)\n",
    "ax.plot(history_camil['val_loss'], 'g--', label='CAMIL Val', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Training & Validation Loss', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax = axes[1]\n",
    "ax.plot([a*100 for a in history_baseline['train_acc']], 'r-', label='Baseline Train', linewidth=2)\n",
    "ax.plot([a*100 for a in history_baseline['val_acc']], 'r--', label='Baseline Val', linewidth=2)\n",
    "ax.plot([a*100 for a in history_camil['train_acc']], 'g-', label='CAMIL Train', linewidth=2)\n",
    "ax.plot([a*100 for a in history_camil['val_acc']], 'g--', label='CAMIL Val', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Training & Validation Accuracy', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "curves_path = f\"{SAVE_DIR}/training_curves_proxy.png\"\n",
    "plt.savefig(curves_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"üíæ Sauvegard√©: {curves_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664737b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap d'attention\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "n_samples = min(20, len(attention_baseline))\n",
    "\n",
    "ax = axes[0]\n",
    "im = ax.imshow(attention_baseline[:n_samples], aspect='auto', cmap='Reds')\n",
    "ax.set_ylabel('Sample', fontsize=12)\n",
    "ax.set_title('Baseline - Attention UNIFORME (1/T pour chaque frame)', fontsize=14)\n",
    "plt.colorbar(im, ax=ax, label='Poids')\n",
    "\n",
    "ax = axes[1]\n",
    "im = ax.imshow(attention_camil[:n_samples], aspect='auto', cmap='Greens')\n",
    "ax.set_xlabel('Slice (axe Z du volume)', fontsize=12)\n",
    "ax.set_ylabel('Sample', fontsize=12)\n",
    "ax.set_title('MedViT-CAMIL - Attention APPRISE (focus sur slices informatives)', fontsize=14)\n",
    "plt.colorbar(im, ax=ax, label='Poids')\n",
    "\n",
    "plt.tight_layout()\n",
    "heatmap_path = f\"{SAVE_DIR}/attention_heatmap_proxy.png\"\n",
    "plt.savefig(heatmap_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"üíæ Sauvegard√©: {heatmap_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebc1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des poids d'attention\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "mean_baseline = attention_baseline.mean(axis=0)\n",
    "ax.bar(range(len(mean_baseline)), mean_baseline, color='red', alpha=0.7)\n",
    "ax.axhline(y=1/len(mean_baseline), color='black', linestyle='--', label='Uniforme')\n",
    "ax.set_xlabel('Slice', fontsize=12)\n",
    "ax.set_ylabel('Poids moyen', fontsize=12)\n",
    "ax.set_title('Baseline: Distribution uniforme', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "mean_camil = attention_camil.mean(axis=0)\n",
    "ax.bar(range(len(mean_camil)), mean_camil, color='green', alpha=0.7)\n",
    "ax.axhline(y=1/len(mean_camil), color='black', linestyle='--', label='Uniforme')\n",
    "ax.set_xlabel('Slice', fontsize=12)\n",
    "ax.set_ylabel('Poids moyen', fontsize=12)\n",
    "ax.set_title('CAMIL: Focus sur slices centrales (r√©gion nodulaire)', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "dist_path = f\"{SAVE_DIR}/attention_distribution_proxy.png\"\n",
    "plt.savefig(dist_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"üíæ Sauvegard√©: {dist_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bae1c6",
   "metadata": {},
   "source": [
    "## üìã R√©sum√© Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_message(f\"\\n{'='*60}\")\n",
    "log_message(\"üìã R√âSUM√â FINAL\")\n",
    "log_message(f\"{'='*60}\")\n",
    "log_message(f\"Termin√©: {datetime.now()}\")\n",
    "log_message(f\"\")\n",
    "log_message(f\"M√âTRIQUES:\")\n",
    "log_message(f\"  Baseline Test Accuracy: {test_acc_baseline*100:.2f}%\")\n",
    "log_message(f\"  CAMIL Test Accuracy:    {test_acc_camil*100:.2f}%\")\n",
    "log_message(f\"  Am√©lioration:           {improvement:+.2f}%\")\n",
    "log_message(f\"\")\n",
    "log_message(f\"FICHIERS SAUVEGARD√âS dans {SAVE_DIR}:\")\n",
    "log_message(f\"  üìÑ config.json\")\n",
    "log_message(f\"  üìÑ results_proxy.json\")\n",
    "log_message(f\"  üìÑ training_log.txt\")\n",
    "log_message(f\"  üñºÔ∏è training_curves_proxy.png\")\n",
    "log_message(f\"  üñºÔ∏è attention_heatmap_proxy.png\")\n",
    "log_message(f\"  üñºÔ∏è attention_distribution_proxy.png\")\n",
    "log_message(f\"  ü§ñ Baseline-AvgPool_best.pth\")\n",
    "log_message(f\"  ü§ñ MedViT-CAMIL_best.pth\")\n",
    "log_message(f\"  üìÑ Baseline-AvgPool_history.json\")\n",
    "log_message(f\"  üìÑ MedViT-CAMIL_history.json\")\n",
    "log_message(f\"{'='*60}\")\n",
    "\n",
    "# Lister les fichiers\n",
    "print(\"\\nüìÅ Fichiers dans Google Drive:\")\n",
    "!ls -la $SAVE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714a1b6",
   "metadata": {},
   "source": [
    "## üì• Instructions pour r√©cup√©rer les r√©sultats\n",
    "\n",
    "Les fichiers sont automatiquement sauvegard√©s dans **Google Drive** :\n",
    "`/MyDrive/MedViT_Results/proxy/`\n",
    "\n",
    "### Pour analyser les r√©sultats avec l'assistant:\n",
    "1. Va dans Google Drive ‚Üí MedViT_Results ‚Üí proxy\n",
    "2. T√©l√©charge les fichiers:\n",
    "   - `results_proxy.json` (m√©triques)\n",
    "   - `training_curves_proxy.png` (courbes)\n",
    "   - `attention_heatmap_proxy.png` (attention)\n",
    "3. D√©pose-les dans le dossier `results/` de ton projet local\n",
    "4. Demande √† l'assistant d'analyser les r√©sultats!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

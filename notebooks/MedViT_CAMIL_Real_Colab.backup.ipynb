{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a47b7c",
   "metadata": {},
   "source": [
    "# üè• MedViT-CAMIL: Mode REAL (HyperKvasir Videos)\n",
    "\n",
    "**Context-Aware Multiple Instance Learning for Medical Video Analysis**\n",
    "\n",
    "Ce notebook ex√©cute le mode REAL sur Google Colab avec GPU.\n",
    "\n",
    "- **Dataset**: HyperKvasir Labeled Videos (~25 GB d'endoscopies)\n",
    "- **T√¢che**: Classification binaire (normal/pathologique)\n",
    "- **Comparaison**: Baseline (Average Pooling) vs MedViT-CAMIL (Gated Attention)\n",
    "\n",
    "---\n",
    "‚ö° **IMPORTANT**: \n",
    "1. Activez le GPU: `Runtime > Change runtime type > T4 GPU`\n",
    "2. Le dataset est volumineux (~25 GB), pr√©voir du temps de t√©l√©chargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e078b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le GPU et l'espace disque\n",
    "!nvidia-smi\n",
    "!df -h /content\n",
    "\n",
    "import torch\n",
    "print(f\"\\n‚úÖ PyTorch {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances\n",
    "!pip install -q timm tqdm matplotlib seaborn einops opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7ad6d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ T√©l√©chargement du Dataset HyperKvasir Videos\n",
    "\n",
    "**HyperKvasir** est un dataset d'endoscopies GI du Simula Research Lab.\n",
    "- 374 vid√©os labellis√©es de proc√©dures gastro-intestinales\n",
    "- Classes: findings normaux vs pathologiques (polypes, ulc√®res, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DATA_DIR = '/content/data/hyperkvasir'\n",
    "ZIP_URL = 'https://datasets.simula.no/downloads/hyper-kvasir/hyper-kvasir-labeled-videos.zip'\n",
    "ZIP_PATH = '/content/hyper-kvasir-labeled-videos.zip'\n",
    "\n",
    "# T√©l√©charger si n√©cessaire\n",
    "if not os.path.exists(DATA_DIR) or len(os.listdir(DATA_DIR)) == 0:\n",
    "    print(\"üì• T√©l√©chargement de HyperKvasir Videos (~25 GB)...\")\n",
    "    print(\"‚è≥ Cela peut prendre 10-20 minutes selon la connexion...\")\n",
    "    !wget -q --show-progress -O $ZIP_PATH $ZIP_URL\n",
    "    \n",
    "    print(\"\\nüì¶ Extraction de l'archive...\")\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content/data/extracted')\n",
    "    \n",
    "    # Organiser en normal/abnormal\n",
    "    print(\"üìÅ Organisation des donn√©es...\")\n",
    "    os.makedirs(f\"{DATA_DIR}/normal\", exist_ok=True)\n",
    "    os.makedirs(f\"{DATA_DIR}/abnormal\", exist_ok=True)\n",
    "    \n",
    "    # Chercher les vid√©os et les classer\n",
    "    import shutil\n",
    "    import glob\n",
    "    \n",
    "    NORMAL_KEYWORDS = ['normal', 'cecum', 'pylorus', 'z-line', 'retroflex']\n",
    "    ABNORMAL_KEYWORDS = ['polyp', 'ulcer', 'esophagitis', 'colitis', 'hemorrhoid', 'dyed']\n",
    "    \n",
    "    for video_path in glob.glob('/content/data/extracted/**/*.mp4', recursive=True):\n",
    "        folder_name = os.path.dirname(video_path).lower()\n",
    "        file_name = os.path.basename(video_path)\n",
    "        \n",
    "        is_normal = any(kw in folder_name for kw in NORMAL_KEYWORDS)\n",
    "        is_abnormal = any(kw in folder_name for kw in ABNORMAL_KEYWORDS)\n",
    "        \n",
    "        if is_abnormal:\n",
    "            dest = f\"{DATA_DIR}/abnormal/{file_name}\"\n",
    "        elif is_normal:\n",
    "            dest = f\"{DATA_DIR}/normal/{file_name}\"\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if not os.path.exists(dest):\n",
    "            shutil.copy2(video_path, dest)\n",
    "    \n",
    "    # Nettoyage\n",
    "    !rm -rf /content/data/extracted\n",
    "    !rm -f $ZIP_PATH\n",
    "    \n",
    "# Compter les fichiers\n",
    "normal_count = len(glob.glob(f\"{DATA_DIR}/normal/*.mp4\"))\n",
    "abnormal_count = len(glob.glob(f\"{DATA_DIR}/abnormal/*.mp4\"))\n",
    "print(f\"\\n‚úÖ Dataset pr√™t:\")\n",
    "print(f\"   - Normal: {normal_count} vid√©os\")\n",
    "print(f\"   - Abnormal: {abnormal_count} vid√©os\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06320a87",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration MODE REAL\n",
    "CONFIG = {\n",
    "    'MODE': 'real',\n",
    "    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'SEQ_LEN': 32,          # Frames extraites par vid√©o\n",
    "    'IMG_SIZE': 224,\n",
    "    'BATCH_SIZE': 8,        # R√©duit car vid√©os lourdes\n",
    "    'EPOCHS': 30,\n",
    "    'LR': 1e-4,\n",
    "    'WEIGHT_DECAY': 1e-5,\n",
    "    'NUM_CLASSES': 2,\n",
    "    'HIDDEN_DIM': 128,\n",
    "    'SEED': 42,\n",
    "    'DATA_DIR': '/content/data/hyperkvasir'\n",
    "}\n",
    "\n",
    "# Reproductibilit√©\n",
    "torch.manual_seed(CONFIG['SEED'])\n",
    "np.random.seed(CONFIG['SEED'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CONFIG['SEED'])\n",
    "\n",
    "print(\"üìã CONFIGURATION REAL\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614fef6",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Dataset: Real Video Loader (OpenCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a786bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "class RealVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset pour vraies vid√©os m√©dicales via OpenCV.\n",
    "    Extrait SEQ_LEN frames uniform√©ment r√©parties dans chaque vid√©o.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, split='train', seq_len=32, img_size=224):\n",
    "        self.seq_len = seq_len\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        ])\n",
    "        \n",
    "        # Scanner les fichiers\n",
    "        self.samples = []\n",
    "        for label, cls in enumerate(['normal', 'abnormal']):\n",
    "            cls_path = os.path.join(data_dir, cls)\n",
    "            if os.path.exists(cls_path):\n",
    "                for f in glob.glob(os.path.join(cls_path, '*.mp4')):\n",
    "                    self.samples.append((f, label))\n",
    "        \n",
    "        # Shuffle et split 80/20\n",
    "        np.random.shuffle(self.samples)\n",
    "        cut = int(0.8 * len(self.samples))\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.samples = self.samples[:cut]\n",
    "        else:\n",
    "            self.samples = self.samples[cut:]\n",
    "        \n",
    "        print(f\"[{split.upper()}] {len(self.samples)} vid√©os charg√©es\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def _extract_frames(self, video_path):\n",
    "        \"\"\"Extrait SEQ_LEN frames uniform√©ment de la vid√©o.\"\"\"\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total_frames == 0:\n",
    "            cap.release()\n",
    "            return None\n",
    "        \n",
    "        # Indices √† extraire (uniform√©ment r√©partis)\n",
    "        indices = np.linspace(0, total_frames - 1, self.seq_len, dtype=int)\n",
    "        \n",
    "        frame_idx = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            if frame_idx in indices:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(self.transform(frame))\n",
    "            \n",
    "            frame_idx += 1\n",
    "            if len(frames) >= self.seq_len:\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Padding si n√©cessaire\n",
    "        if len(frames) == 0:\n",
    "            return None\n",
    "        \n",
    "        while len(frames) < self.seq_len:\n",
    "            frames.append(frames[-1])  # R√©p√©ter la derni√®re frame\n",
    "        \n",
    "        return torch.stack(frames[:self.seq_len])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            video = self._extract_frames(path)\n",
    "            if video is None:\n",
    "                video = torch.zeros(self.seq_len, 3, self.img_size, self.img_size)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Erreur {path}: {e}\")\n",
    "            video = torch.zeros(self.seq_len, 3, self.img_size, self.img_size)\n",
    "        \n",
    "        return video, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Cr√©er les datasets\n",
    "train_dataset = RealVideoDataset(CONFIG['DATA_DIR'], 'train', CONFIG['SEQ_LEN'], CONFIG['IMG_SIZE'])\n",
    "val_dataset = RealVideoDataset(CONFIG['DATA_DIR'], 'val', CONFIG['SEQ_LEN'], CONFIG['IMG_SIZE'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"\\nüìä Train: {len(train_dataset)} | Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d74f1a",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Mod√®les: Backbone MobileViT + Aggregateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24778287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class MobileViTBackbone(nn.Module):\n",
    "    \"\"\"Backbone MobileViT pr√©-entra√Æn√© (GEL√â).\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='mobilevit_s', pretrained=True):\n",
    "        super().__init__()\n",
    "        print(f\"[INFO] Chargement {model_name}...\")\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained, \n",
    "                                          num_classes=0, global_pool='avg')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            self.feature_dim = self.backbone(dummy).shape[-1]\n",
    "        \n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.backbone.eval()\n",
    "        print(f\"[INFO] Feature dim: {self.feature_dim}, Backbone GEL√â\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(x)\n",
    "        return features.view(B, T, -1)\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        self.backbone.eval()\n",
    "        return self\n",
    "\n",
    "\n",
    "class BaselineAvgPooling(nn.Module):\n",
    "    \"\"\"Baseline: Moyenne temporelle simple.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim, hidden_dim=128, num_classes=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        B, T, D = features.shape\n",
    "        projected = self.projection(features)\n",
    "        aggregated = projected.mean(dim=1)\n",
    "        logits = self.classifier(aggregated)\n",
    "        attention = torch.ones(B, T, device=features.device) / T\n",
    "        return logits, attention\n",
    "\n",
    "\n",
    "class ContextAwareGatedMIL(nn.Module):\n",
    "    \"\"\"CAMIL: Context-Aware Gated Attention MIL.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim, hidden_dim=128, num_classes=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.context_conv = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.attention_V = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.Tanh())\n",
    "        self.attention_U = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.Sigmoid())\n",
    "        self.attention_w = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        B, T, D = features.shape\n",
    "        \n",
    "        h = self.input_projection(features)\n",
    "        h_conv = self.context_conv(h.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        h = h + h_conv\n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        v = self.attention_V(h)\n",
    "        u = self.attention_U(h)\n",
    "        gated = v * u\n",
    "        \n",
    "        attention_scores = self.attention_w(gated).squeeze(-1)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        aggregated = torch.bmm(attention_weights.unsqueeze(1), h).squeeze(1)\n",
    "        logits = self.classifier(aggregated)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "\n",
    "class MedViTModel(nn.Module):\n",
    "    \"\"\"Mod√®le complet: Backbone + Aggregateur.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_camil=True, hidden_dim=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = MobileViTBackbone()\n",
    "        feature_dim = self.backbone.feature_dim\n",
    "        \n",
    "        if use_camil:\n",
    "            self.aggregator = ContextAwareGatedMIL(feature_dim, hidden_dim, num_classes)\n",
    "            self.name = \"MedViT-CAMIL\"\n",
    "        else:\n",
    "            self.aggregator = BaselineAvgPooling(feature_dim, hidden_dim, num_classes)\n",
    "            self.name = \"Baseline-AvgPool\"\n",
    "    \n",
    "    def forward(self, video):\n",
    "        features = self.backbone(video)\n",
    "        return self.aggregator(features)\n",
    "\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8975e85c",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Fonctions d'entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d90d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for videos, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(videos)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, pred = logits.max(1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_attention = []\n",
    "    \n",
    "    for videos, labels in loader:\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "        logits, attention = model(videos)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, pred = logits.max(1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_attention.append(attention.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), correct / total, np.concatenate(all_attention)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config):\n",
    "    device = config['DEVICE']\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total, trainable = count_params(model)\n",
    "    print(f\"\\nüîß {model.name}\")\n",
    "    print(f\"   Params: {total:,} total, {trainable:,} entra√Ænables\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=config['LR'], weight_decay=config['WEIGHT_DECAY']\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config['EPOCHS'])\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(config['EPOCHS']):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _ = evaluate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'{model.name}_best.pth')\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config['EPOCHS']} | \"\n",
    "              f\"Train: {train_loss:.4f} / {train_acc*100:.1f}% | \"\n",
    "              f\"Val: {val_loss:.4f} / {val_acc*100:.1f}%\")\n",
    "    \n",
    "    print(f\"‚úÖ Best Val Accuracy: {best_acc*100:.2f}%\")\n",
    "    return model, history, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a743a695",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Ex√©cution: Baseline vs CAMIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üèÉ ENTRA√éNEMENT BASELINE (Average Pooling)\")\n",
    "print(\"=\"*60)\n",
    "model_baseline = MedViTModel(use_camil=False, hidden_dim=CONFIG['HIDDEN_DIM'], num_classes=CONFIG['NUM_CLASSES'])\n",
    "model_baseline, history_baseline, best_baseline = train_model(model_baseline, train_loader, val_loader, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803ab2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üèÉ ENTRA√éNEMENT MedViT-CAMIL (Gated Attention)\")\n",
    "print(\"=\"*60)\n",
    "model_camil = MedViTModel(use_camil=True, hidden_dim=CONFIG['HIDDEN_DIM'], num_classes=CONFIG['NUM_CLASSES'])\n",
    "model_camil, history_camil, best_camil = train_model(model_camil, train_loader, val_loader, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745915f",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ R√©sultats Finaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a699652",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = CONFIG['DEVICE']\n",
    "\n",
    "# Charger les meilleurs mod√®les\n",
    "model_baseline.load_state_dict(torch.load('Baseline-AvgPool_best.pth'))\n",
    "model_camil.load_state_dict(torch.load('MedViT-CAMIL_best.pth'))\n",
    "\n",
    "# √âvaluer\n",
    "_, val_acc_baseline, attention_baseline = evaluate(model_baseline, val_loader, criterion, device)\n",
    "_, val_acc_camil, attention_camil = evaluate(model_camil, val_loader, criterion, device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä R√âSULTATS FINAUX (HyperKvasir Videos)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Baseline (Avg Pool):  {val_acc_baseline*100:.2f}%\")\n",
    "print(f\"MedViT-CAMIL:         {val_acc_camil*100:.2f}%\")\n",
    "print(f\"Am√©lioration:         {(val_acc_camil - val_acc_baseline)*100:+.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d844fd8b",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax = axes[0]\n",
    "ax.plot(history_baseline['train_loss'], 'r-', label='Baseline Train')\n",
    "ax.plot(history_baseline['val_loss'], 'r--', label='Baseline Val')\n",
    "ax.plot(history_camil['train_loss'], 'g-', label='CAMIL Train')\n",
    "ax.plot(history_camil['val_loss'], 'g--', label='CAMIL Val')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training & Validation Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax = axes[1]\n",
    "ax.plot([a*100 for a in history_baseline['train_acc']], 'r-', label='Baseline Train')\n",
    "ax.plot([a*100 for a in history_baseline['val_acc']], 'r--', label='Baseline Val')\n",
    "ax.plot([a*100 for a in history_camil['train_acc']], 'g-', label='CAMIL Train')\n",
    "ax.plot([a*100 for a in history_camil['val_acc']], 'g--', label='CAMIL Val')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Training & Validation Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves_real.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap d'attention\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "n_samples = min(15, len(attention_baseline))\n",
    "\n",
    "ax = axes[0]\n",
    "im = ax.imshow(attention_baseline[:n_samples], aspect='auto', cmap='Reds')\n",
    "ax.set_ylabel('Vid√©o')\n",
    "ax.set_title('Baseline - Attention UNIFORME (dilue le signal temporel)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "ax = axes[1]\n",
    "im = ax.imshow(attention_camil[:n_samples], aspect='auto', cmap='Greens')\n",
    "ax.set_xlabel('Frame (temps)')\n",
    "ax.set_ylabel('Vid√©o')\n",
    "ax.set_title('MedViT-CAMIL - Attention APPRISE (pics sur moments cl√©s)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('attention_heatmap_real.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7f0e7",
   "metadata": {},
   "source": [
    "## üì• T√©l√©charger les r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb56e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = {\n",
    "    'mode': 'real',\n",
    "    'dataset': 'HyperKvasir-Videos',\n",
    "    'config': {k: str(v) if isinstance(v, torch.device) else v for k, v in CONFIG.items()},\n",
    "    'results': {\n",
    "        'baseline': {'val_accuracy': val_acc_baseline},\n",
    "        'camil': {'val_accuracy': val_acc_camil},\n",
    "        'improvement': val_acc_camil - val_acc_baseline\n",
    "    },\n",
    "    'history': {\n",
    "        'baseline': history_baseline,\n",
    "        'camil': history_camil\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('results_real.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Fichiers sauvegard√©s:\")\n",
    "print(\"   - results_real.json\")\n",
    "print(\"   - training_curves_real.png\")\n",
    "print(\"   - attention_heatmap_real.png\")\n",
    "print(\"   - Baseline-AvgPool_best.pth\")\n",
    "print(\"   - MedViT-CAMIL_best.pth\")\n",
    "\n",
    "# Pour t√©l√©charger sur Colab\n",
    "from google.colab import files\n",
    "files.download('results_real.json')\n",
    "files.download('training_curves_real.png')\n",
    "files.download('attention_heatmap_real.png')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
